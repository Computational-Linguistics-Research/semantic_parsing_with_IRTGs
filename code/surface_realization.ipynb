{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surface import grammar\n",
    "from surface import converter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_structure(graph_data, word_id):\n",
    "    if word_id not in graph_data:\n",
    "        graph_data[word_id] = {\n",
    "            \"word\": \"\",\n",
    "            \"deps\": {},\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rules(dev):\n",
    "    graph_data = {}\n",
    "    noun_list = []\n",
    "    id_to_rules = defaultdict(list)\n",
    "    id_to_sentence = {}\n",
    "    sentences = 0\n",
    "    with open(dev, \"r\") as f:\n",
    "        for i,line in enumerate(f):            \n",
    "            if line == \"\\n\":\n",
    "                words = []\n",
    "                for w in graph_data:\n",
    "                    words.append(graph_data[w][\"word\"])\n",
    "                    subgraphs = {\"root\": None, \"graph\": []}\n",
    "                    rules = []\n",
    "                    if \"tree_pos\" not in graph_data[w]:\n",
    "                        continue\n",
    "                    \n",
    "                    subgraphs[\"root\"] = graph_data[w][\"tree_pos\"]\n",
    "                    \n",
    "                    for dep in graph_data[w][\"deps\"]:                        \n",
    "                        edge_dep = graph_data[w][\"deps\"][dep]\n",
    "                        to_pos = graph_data[dep][\"tree_pos\"]\n",
    "                        mor = graph_data[dep][\"mor\"]\n",
    "                            \n",
    "                        if \"tree_pos\" in graph_data[w]:\n",
    "                            if \"lin=+\" in mor:\n",
    "                                subgraphs[\"graph\"].append({\"to\":to_pos, \"edge\":edge_dep, \"dir\":\"S\"})\n",
    "                            elif \"lin=-\" in mor:\n",
    "                                subgraphs[\"graph\"].append({\"to\":to_pos, \"edge\":edge_dep, \"dir\":\"B\"})\n",
    "                            else:\n",
    "                                subgraphs[\"graph\"].append({\"to\":to_pos, \"edge\":edge_dep, \"dir\":None})\n",
    "\n",
    "                    id_to_rules[sentences].append(subgraphs)\n",
    "                    id_to_sentence[sentences] = words\n",
    "                graph_data = {}\n",
    "                noun_list = []\n",
    "                sentences += 1\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            if line != \"\\n\":\n",
    "                fields = line.split(\"\\t\")\n",
    "                word_id = fields[0]\n",
    "                word = fields[1]\n",
    "                tree_pos = fields[3]\n",
    "                mor = fields[5]\n",
    "                head = fields[6]\n",
    "                ud_edge = fields[7]\n",
    "\n",
    "                make_default_structure(graph_data, word_id)\n",
    "                graph_data[word_id][\"word\"] = word\n",
    "                graph_data[word_id][\"tree_pos\"] = tree_pos\n",
    "                graph_data[word_id][\"mor\"] = mor\n",
    "\n",
    "                make_default_structure(graph_data, head)\n",
    "                graph_data[head][\"deps\"][word_id] = ud_edge\n",
    "    return id_to_rules, id_to_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR_FILE = \"../count_en_ewt-ud-train.conllu\"\n",
    "TERMINAL_FILE = \"../en_ewt-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Technically',\n",
       " 'was',\n",
       " ',',\n",
       " 'blackberry',\n",
       " '',\n",
       " 'because',\n",
       " 'first',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'with',\n",
       " 'email',\n",
       " 'real',\n",
       " 'and',\n",
       " 'games',\n",
       " 'and',\n",
       " 'stuff',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules, sens = extract_rules(TERMINAL_FILE)\n",
    "sens[1183]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture cap --no-stderr\n",
    "grammar_fn = open('dep_grammar_spec.irtg', 'w') \n",
    "grammar.generate_grammar(GRAMMAR_FILE, rules[1183], grammar_fn)\n",
    "grammar.generate_terminals(TERMINAL_FILE, grammar_fn)\n",
    "grammar_fn.close()\n",
    "#with open('dep_grammar_spec.irtg', 'w') as f:\n",
    "#    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.convert(TERMINAL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs_100 (100 instances) ...\n",
      "01 [[deserved_0/deserved_0 -nsubj-> He_0/He_0; deserve] 509 ms\n",
      "02 [[have_0/have_0 -nsubj-> Anyone_0/Anyone_0; recipe_] 129 ms\n",
      "03 [[arabes_0/arabes_0 -case-> about_0/about_0; arabes] 123 ms\n",
      "04 [[Yes_0/Yes_0 -punct-> PERIOD_0/PERIOD_0]          ] 41 ms\n",
      "05 [[here_0/here_0 -advmod-> Here_0/Here_0; here_0 -ns] 88 ms\n",
      "06 [[cup_0/cup_0 -nummod-> DIGIT_0/DIGIT_0; empanadas_] 62 ms\n",
      "07 [[cup_0/cup_0 -nummod-> DIGIT_0/DIGIT_0; arabes_0/a] 54 ms\n",
      "08 [[cup_0/cup_0 -nummod-> DIGIT_0/DIGIT_0; empanadas_] 65 ms\n",
      "09 [[cup_0/cup_0 -nummod-> DIGIT_0/DIGIT_0; area_0/are] 81 ms\n",
      "10 [[IT_0/IT_0 -det-> ALL_0/ALL_0; MIX_0/MIX_0 -obj-> ] 80 ms\n",
      "11 [[POOP_0/POOP_0 -advmod-> THEN_0/THEN_0; IT_0/IT_0 ] 49 ms\n",
      "12 [[End_0/End_0 -det-> The_0/The_0]                  ] 66 ms\n",
      "13 [[SQs_0/SQs_0 -nsubj-> It_0/It_0; cooking_0/cooking] 92 ms\n",
      "14 [[sent_0/sent_0 -nsubj-> I_0/I_0; phone_0/phone_0 -] 104 ms\n",
      "15 [[sold_0/sold_0 -nsubj_pass-> I_0/I_0; sold_0 -aux_] 100 ms\n",
      "16 [[phone_0/phone_0 -det-> a_0/a_0; phone_0 -amod-> n] 53 ms\n",
      "17 [[call_0/call_0 -nsubj-> they_0/they_0; call_0 -aux] 71 ms\n",
      "18 [[illegal_0/illegal_0 -expl-> it_0/it_0; illegal_0 ] 122 ms\n",
      "19 [[watch_0/watch_0 -discourse-> Erm_0/Erm_0; watch_0] 71 ms\n",
      "20 [[ca_0/ca_0 -nsubj-> Ya_0/Ya_0; ca_0 -advmod-> nSQt] 56 ms\n",
      "21 [[show_0/show_0 -nsubj-> They_0/They_0; show_0 -aux] 72 ms\n",
      "22 [[ca_0/ca_0 -nsubj-> You_0/You_0; ca_0 -advmod-> nS] 278 ms\n",
      "23 [[Caoimhe_0/Caoimhe_0 -nsubj-> Deco_0/Deco_0; Caoim] 38 ms\n",
      "24 [[pregnant_0/pregnant_0 -nsubj-> Suzanne_0/Suzanne_] 60 ms\n",
      "25 [[miss_0/miss_0 -nsubj-> you_0/you_0; miss_0 -aux->] 83 ms\n",
      "26 [[school_0/school_0 -nsubj-> I_0/I_0; school_0 -cop] 165 ms\n",
      "27 [[Sciences_0/Sciences_0 -punct-> HYPHEN_0/HYPHEN_0;] 67 ms\n",
      "28 [[need_0/need_0 -nsubj-> You_0/You_0; background_0/] 65 ms\n",
      "29 [[enough_0/enough_0 -expl-> It_0/It_0; enough_0 -co] 137 ms\n",
      "30 [[adviser_0/adviser_0 -case-> to_0/to_0; adviser_0 ] 121 ms\n",
      "31 [[photography_0/photography_0 -advmod-> certainly_0] 1.037s\n",
      "32 [[httpCOLONPERPERwwwPERIODgooglePERIODcoPERIODukPER] 17 ms\n",
      "33 [[Phone_0/Phone_0 -cop-> Was_0/Was_0; iPhone_0/iPho] 185 ms\n",
      "34 [[writing_0/writing_0 -nsubj-> I_0/I_0; writing_0 -] 354 ms\n",
      "35 [[Was_0/Was_0 -nsubj-> it_0/it_0; Was_0 -punct-> QU] 42 ms\n",
      "36 [[was_0/was_0 -advmod-> Technically_0/Technically_0] 502 ms\n",
      "37 [[phone_0/phone_0 -cc-> But_0/But_0; phone_0 -nsubj] 513 ms\n",
      "38 [[was_0/was_0 -discourse-> No_0/No_0; was_0 -punct-] 40 ms\n",
      "39 [[revolutionize_0/revolutionize_0 -cc-> But_0/But_0] 47 ms\n",
      "40 [[sxDIGITDIGIT_0/sxDIGITDIGIT_0 -compound-> Canon_0] 21 ms\n",
      "41 [[one_0/one_0 -det-> Which_0/Which_0; get_0/get_0 -] 23 ms\n",
      "42 [[DOLLARDOLLARDOLLAR_0/DOLLARDOLLARDOLLAR_0 -nsubj-] 69 ms\n",
      "43 [[SDIGITDIGITDIGIT_0/SDIGITDIGITDIGIT_0 -case-> wit] 20 ms\n",
      "44 [[compact_0/compact_0 -nsubj-> It_0/It_0; compact_0] 583 ms\n",
      "45 [[have_0/have_0 -nsubj-> Both_0/Both_0; video_0/vid] 44 ms\n",
      "46 [[SDIGITDIGITDIGIT_0/SDIGITDIGITDIGIT_0 -det-> The_] 45 ms\n",
      "47 [[needed_0/needed_0 -nsubj_pass-> Passport_0/Passpo] 24 ms\n",
      "48 [[going_0/going_0 -nsubj-> I_0/I_0; going_0 -aux-> ] 34 ms\n",
      "49 [[stay_0/stay_0 -aux-> Will_0/Will_0; waters_0/wate] 38 ms\n",
      "50 [[need_0/need_0 -aux-> Will_0/Will_0; need_0 -nsubj] 24 ms\n",
      "51 [[unsure_0/unsure_0 -mark-> If_0/If_0; unsure_0 -ns] 73 ms\n",
      "52 [[where_0/where_0 -discourse-> uh_0/uh_0; where_0 -] 19 ms\n",
      "53 [[water_0/water_0 -case-> in_0/in_0; water_0 -det->] 19 ms\n",
      "54 [[going_0/going_0 -mark-> where_0/where_0; going_0 ] 18 ms\n",
      "55 [[USA_0/USA_0 -mark-> If_0/If_0; USA_0 -nsubj-> you] 29 ms\n",
      "56 [[get_0/get_0 -advmod-> How_0/How_0; get_0 -aux-> c] 22 ms\n",
      "57 [[tell_0/tell_0 -aux-> Can_0/Can_0; oone_0/oone_0 -] 30 ms\n",
      "58 [[Give_0/Give_0 -iobj-> me_0/me_0; address_0/addres] 24 ms\n",
      "59 [[frustrated_0/frustrated_0 -advmod-> real_0/real_0] 26 ms\n",
      "60 [[Thanks_0/Thanks_0]                               ] 16 ms\n",
      "61 [[asked_0/asked_0 -nsubj-> You_0/You_0; asked_0 -au] 21 ms\n",
      "62 [[looks_0/looks_0 -nsubj-> who_0/who_0; druggy_0/dr] 26 ms\n",
      "63 [[post_0/post_0 -advmod-> Why_0/Why_0; post_0 -aux-] 53 ms\n",
      "64 [[idiot_0/idiot_0 -nsubj-> You_0/You_0; idiot_0 -co] 25 ms\n",
      "65 [[house_0/house_0 -case-> by_0/by_0; house_0 -nmod_] 34 ms\n",
      "66 [[called_0/called_0 -obj-> What_0/What_0; called_0 ] 25 ms\n",
      "67 [[Reel_0/Reel_0 -nsubj-> It_0/It_0; Reel_0 -cop-> s] 33 ms\n",
      "68 [[asked_0/asked_0 -nsubj-> I_0/I_0; asked_0 -aux-> ] 69 ms\n",
      "69 [[provided_0/provided_0 -mark-> With_0/With_0; link] 27 ms\n",
      "70 [[googling_0/googling_0 -obj-> it_0/it_0; Try_0/Try] 21 ms\n",
      "71 [[Link_0/Link_0 -punct-> QUE_0/QUE_0]              ] 18 ms\n",
      "72 [[reel_0/reel_0 -mark-> if_0/if_0; reel_0 -nsubj-> ] 52 ms\n",
      "73 [[restaurant_0/restaurant_0 -det-> a_0/a_0; anniver] 23 ms\n",
      "74 [[anniversary_0/anniversary_0 -nsubj-> It_0/It_0; a] 73 ms\n",
      "75 [[like_0/like_0 -nsubj-> I_0/I_0; Street_0/Street_0] 2.126s\n",
      "76 [[Rib_0/Rib_0 -compound-> Prime_0/Prime_0; Rib_0 -c] 26 ms\n",
      "77 [[expensive_0/expensive_0 -nsubj-> it_0/it_0; expen] 48 ms\n",
      "78 [[happen_0/happen_0 -nsubj-> What_0/What_0; happen_] 43 ms\n",
      "79 [[similar_0/similar_0 -nsubj-> It_0/It_0; similar_0] 57 ms\n",
      "80 [[violating_0/violating_0 -nsubj-> You_0/You_0; vio] 30 ms\n",
      "81 [[police_0/police_0 -det-> The_0/The_0; make_0/make] 61 ms\n",
      "82 [[face_0/face_0 -nsubj-> You_0/You_0; face_0 -aux->] 43 ms\n",
      "83 [[imprisoned_0/imprisoned_0 -nsubj_pass-> They_0/Th] 28 ms\n",
      "84 [[punishment_0/punishment_0 -discourse-> Yes_0/Yes_] 39 ms\n",
      "85 [[poll_0/poll_0 -compound-> Wellington_0/Wellington] 27 ms\n",
      "86 [[vote_0/vote_0 -advmod-> Where_0/Where_0; vote_0 -] 31 ms\n",
      "87 [[voted_0/voted_0 -mark-> If_0/If_0; anyone_0/anyon] 23 ms\n",
      "88 [[vote_0/vote_0 -advmod-> Where_0/Where_0; vote_0 -] 28 ms\n",
      "89 [[want_0/want_0 -mark-> because_0/because_0; want_0] 27 ms\n",
      "90 [[vote_0/vote_0 -cc-> And_0/And_0; voted_0/voted_0 ] 52 ms\n",
      "91 [[closed_0/closed_0 -nsubj-> It_0/It_0; Sunday_0/Su] 29 ms\n",
      "92 [[voted_0/voted_0 -nsubj-> You_0/You_0; website_0/w] 27 ms\n",
      "93 [[two_0/two_0 -det-> The_0/The_0; two_0 -amod-> top] "
     ]
    }
   ],
   "source": [
    "!java -Xmx8G -cp alto-2.3.1-all.jar de.up.ling.irtg.script.ParsingEvaluator graphs_100 -g dep_grammar_spec.irtg -I ud -O string=toString -o surface_eval_ewt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
